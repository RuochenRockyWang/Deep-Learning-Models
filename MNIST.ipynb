{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3307354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322834\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.236447\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.118064\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.793712\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.639150\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.522026\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.444431\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.304309\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.276492\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.224129\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.445756\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.395542\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.208383\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.061456\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.185824\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.252865\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.312695\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.558231\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.174176\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.231722\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.031564\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.131559\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.269566\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.075500\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.186041\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.419029\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.179157\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.015059\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.159220\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.068969\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.929735\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.210596\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.770386\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.976742\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.069313\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.826249\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.830500\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.101418\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.949216\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.897911\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.774238\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.108921\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.940662\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.903798\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.993506\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.033760\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.006302\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.931965\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.016923\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.800978\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.099335\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.899760\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.672588\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.837039\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.234354\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.004234\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.922290\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.795659\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.844030\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.872578\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.925093\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.824945\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.937469\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.845586\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.969253\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.650380\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.601689\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.544813\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.675276\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.565502\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.536956\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.915906\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.711116\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.603106\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.659898\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.706219\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.616094\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.751286\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.594719\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.788721\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.528447\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.534589\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.512555\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.580065\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.620708\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.606847\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.240968\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.608841\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.558464\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.594212\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.538954\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.647941\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.534916\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.608589\n",
      "\n",
      "Test set: Average loss: 0.4796, Accuracy: 8402/10000 (84%)\n",
      "\n",
      "finish epoch 1\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.524753\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.623432\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.557362\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.527829\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.469730\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.545120\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.508628\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.652088\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.442574\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.817727\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.452119\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.327238\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.557077\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.386596\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.488561\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.584324\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.456098\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.417567\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.440275\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.668402\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.873458\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.456321\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.410794\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.533597\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.555744\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.384478\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.336198\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.436236\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.353919\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.531323\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.300346\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.316891\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.254824\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.268374\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.524484\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.426839\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.711588\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.413405\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.663626\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.692809\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.514832\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.349121\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.456955\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.502217\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.491483\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.594095\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.262703\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.446222\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.397967\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.503730\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.266139\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.276771\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.567039\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.406527\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.618347\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.414465\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.442019\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.364562\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.572645\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.388657\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.790435\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.260081\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.321933\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.517286\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.375497\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.350647\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.362036\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.183068\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.204072\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.368204\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.186036\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.136684\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.171328\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.297881\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.383144\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.373574\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.482605\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.346711\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.288538\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.164620\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.385180\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.295487\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.417036\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.478427\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.396668\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.359756\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.279532\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.302414\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.217031\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.441175\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.413491\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.159436\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.370048\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.171027\n",
      "\n",
      "Test set: Average loss: 0.2769, Accuracy: 9196/10000 (92%)\n",
      "\n",
      "finish epoch 2\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.239880\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.412209\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.406520\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.175558\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.352678\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.282225\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.095203\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.335306\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.439919\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.259969\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.356807\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.424839\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.251311\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.278668\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.431004\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.238462\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.683271\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.207867\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.342208\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.327386\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.210380\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.447371\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.324927\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.225301\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.197426\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.232122\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.444261\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.426749\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.282797\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.275097\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.279702\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.170369\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.282407\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.295761\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.198240\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.342401\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.273938\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.292699\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.297860\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.206731\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.422991\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.295220\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.184617\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.076496\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.417731\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.384390\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.242694\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.306659\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.246412\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.411852\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.185085\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.244184\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.315241\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.351644\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.346099\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.428048\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.165404\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.232281\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.300575\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.157769\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.320778\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.396910\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.119609\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.220716\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.346701\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.368045\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.640486\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.289370\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.128929\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.285354\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.417861\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.193317\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.103449\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.252643\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.125975\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.554986\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.380363\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.200975\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.443011\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.268930\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.277857\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.324384\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.458394\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.127726\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.192392\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.366174\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.195214\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.212479\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.125613\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.255403\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.117626\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.166272\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.116651\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.116176\n",
      "\n",
      "Test set: Average loss: 0.2426, Accuracy: 9337/10000 (93%)\n",
      "\n",
      "finish epoch 3\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.296531\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.314239\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.280091\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.190880\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.175661\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.251212\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.322417\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.245568\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.135044\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.274170\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.242844\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.177417\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.171152\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.371733\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.287757\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.371807\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.256654\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.108121\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.276761\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.194402\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.248370\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.249368\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.180468\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.134511\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.156538\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.164467\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.283896\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.185870\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.074413\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.241357\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.196356\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.236925\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.375929\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.178814\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.272193\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.260230\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.266588\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.376782\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.453689\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.163226\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.318667\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.236498\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.186008\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.261536\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.167856\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.349717\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.324852\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.167426\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.135625\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.399855\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.603417\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.133468\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.262429\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.230454\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.204005\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.290113\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.148726\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.266658\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.079156\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.241723\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.234031\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.120096\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.243424\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.225498\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.258938\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.130972\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.276758\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.258957\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.227307\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.364283\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.394079\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.347545\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.297776\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.302797\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.179287\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.164257\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.148708\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.030999\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.115144\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.128272\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.362292\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.292259\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.312569\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.196192\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.320274\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.121957\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.047502\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.139087\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.325046\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.198447\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.277278\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.298387\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.157899\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.281939\n",
      "\n",
      "Test set: Average loss: 0.2236, Accuracy: 9363/10000 (94%)\n",
      "\n",
      "finish epoch 4\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.355659\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.102601\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.063752\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.083671\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.218972\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.273730\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Assignment_2_Part_2_RNN_MNIST_vp1.ipynb\n",
    "Overall structure:\n",
    "\n",
    "1) Set Pytorch metada\n",
    "- seed\n",
    "- tensorflow output\n",
    "- whether to transfer to gpu (cuda)\n",
    "\n",
    "2) Import data\n",
    "- download data\n",
    "- create data loaders with batchsie, transforms, scaling\n",
    "\n",
    "3) Define Model architecture, loss and optimizer\n",
    "\n",
    "4) Define Test and Training loop\n",
    "    - Train:\n",
    "        a. get next batch\n",
    "        b. forward pass through model\n",
    "        c. calculate loss\n",
    "        d. backward pass from loss (calculates the gradient for each parameter)\n",
    "        e. optimizer: performs weight updates\n",
    "\n",
    "5) Perform Training over multiple epochs:\n",
    "    Each epoch:\n",
    "    - call train loop\n",
    "    - call test loop\n",
    "\n",
    "# Step 1: Pytorch and Training Metadata\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "# 1) setting up the logging\n",
    "\n",
    "datetime_str = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "if logging_dir is None:\n",
    "    runs_dir = Path(\"./\") / Path(f\"runs/\")\n",
    "    runs_dir.mkdir(exist_ok = True)\n",
    "\n",
    "    logging_dir = runs_dir / Path(f\"{datetime_str}\")\n",
    "\n",
    "    logging_dir.mkdir(exist_ok = True)\n",
    "    logging_dir = str(logging_dir.absolute())\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)\n",
    "\n",
    "#deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "\"\"\"# Step 2: Data Setup\"\"\"\n",
    "\n",
    "# Setting up data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# plot one example\n",
    "print(train_dataset.data.size())     # (60000, 28, 28)\n",
    "print(train_dataset.targets.size())   # (60000)\n",
    "plt.imshow(train_dataset.data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_dataset.targets[0])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"# Step 3: Creating the Model\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,     # number of hidden units\n",
    "            num_layers=2,       # number of layers\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(64, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, hidden = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"# Step 4: Train/Test\"\"\"\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data = data.view(-1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            writer.add_scalar('training_loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "    writer.add_scalar('test_accuracy', 100. * correct / len(test_loader.dataset), epoch)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    print('finish epoch',epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e51aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Assignment_2_Part_2_RNN_MNIST_vp1.ipynb\n",
    "Overall structure:\n",
    "\n",
    "1) Set Pytorch metada\n",
    "- seed\n",
    "- tensorflow output\n",
    "- whether to transfer to gpu (cuda)\n",
    "\n",
    "2) Import data\n",
    "- download data\n",
    "- create data loaders with batchsie, transforms, scaling\n",
    "\n",
    "3) Define Model architecture, loss and optimizer\n",
    "\n",
    "4) Define Test and Training loop\n",
    "    - Train:\n",
    "        a. get next batch\n",
    "        b. forward pass through model\n",
    "        c. calculate loss\n",
    "        d. backward pass from loss (calculates the gradient for each parameter)\n",
    "        e. optimizer: performs weight updates\n",
    "\n",
    "5) Perform Training over multiple epochs:\n",
    "    Each epoch:\n",
    "    - call train loop\n",
    "    - call test loop\n",
    "\n",
    "# Step 1: Pytorch and Training Metadata\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "# 1) setting up the logging\n",
    "\n",
    "datetime_str = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "if logging_dir is None:\n",
    "    runs_dir = Path(\"./\") / Path(f\"runs/\")\n",
    "    runs_dir.mkdir(exist_ok = True)\n",
    "\n",
    "    logging_dir = runs_dir / Path(f\"{datetime_str}\")\n",
    "\n",
    "    logging_dir.mkdir(exist_ok = True)\n",
    "    logging_dir = str(logging_dir.absolute())\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)\n",
    "\n",
    "#deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "\"\"\"# Step 2: Data Setup\"\"\"\n",
    "\n",
    "# Setting up data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# plot one example\n",
    "print(train_dataset.data.size())     # (60000, 28, 28)\n",
    "print(train_dataset.targets.size())   # (60000)\n",
    "plt.imshow(train_dataset.data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_dataset.targets[0])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"# Step 3: Creating the Model\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Using LSTM instead of RNN\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,     # number of hidden units\n",
    "            num_layers=1,       # number of layers\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(128, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM returns output, (h_n, c_n)\n",
    "        r_out, (h_n, c_n) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"# Step 4: Train/Test\"\"\"\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data = data.view(-1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            writer.add_scalar('training_loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "    writer.add_scalar('test_accuracy', 100. * correct / len(test_loader.dataset), epoch)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    print('finish epoch',epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0959a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.330496\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.247383\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.040808\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.785809\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.716329\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.383663\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.117331\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.883184\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.792121\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.882863\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.693981\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.558631\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.621032\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.643279\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.583692\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.359569\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.374760\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.137722\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.528172\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.242873\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.515294\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.373094\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.316222\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.463847\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.262383\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.533997\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.321946\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.294793\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.277587\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.111773\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.260335\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.301066\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.523657\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.147076\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.240224\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.283858\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.162856\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.456734\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.156816\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.082928\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.149504\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.217817\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.112084\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.103955\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.131530\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.301175\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.144330\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.185312\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.112093\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.084446\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.125175\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.269883\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.201492\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.193633\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.227287\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.161380\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.110605\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.140309\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.054904\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.063861\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.169539\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.120498\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.229538\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.145854\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.147230\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.054727\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.120389\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.104655\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.080786\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.026604\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.206136\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.120822\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.105820\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.097141\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.272757\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.111606\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.041062\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.054803\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.210525\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.032883\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.099400\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.021130\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.211846\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.068511\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.048694\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.044725\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.109024\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.252631\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.090642\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.124229\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.086986\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.064151\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.133976\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.020359\n",
      "\n",
      "Test set: Average loss: 0.0862, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "finish epoch 1\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.062212\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.024777\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.024210\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.165984\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.170657\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.084132\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.142216\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.032862\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.081627\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.094393\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.203406\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.143131\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.099503\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.055626\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.155218\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.040841\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.051506\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.222754\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.121978\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.044581\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.035627\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.022889\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.095325\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.109615\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.085793\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.235982\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.042600\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.102048\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.180941\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.019124\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.063379\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.015418\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.076414\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.186429\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.025562\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.081556\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.024131\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.053841\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.122017\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.057835\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.169279\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.057960\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.028010\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.029866\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.026136\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.080306\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.126465\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.022272\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.027130\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.074980\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.165206\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.026201\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.014327\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.018449\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.064683\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.104481\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.098034\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.119794\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.103255\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.086484\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.045650\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.115147\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.034988\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.212637\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.041714\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.110725\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.210289\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.042142\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.060083\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.067123\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.068434\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.101465\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.045967\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.019192\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.101490\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.071732\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.028628\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.017276\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.059123\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.003667\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.106117\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.017958\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.143576\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.072628\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.048790\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.027088\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.037469\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.045276\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.061981\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.087330\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.118612\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.036244\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.035696\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.141827\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 9780/10000 (98%)\n",
      "\n",
      "finish epoch 2\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.051577\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.008345\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.093785\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.050491\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.054056\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.032316\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.010650\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.056008\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.019777\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.022126\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.049372\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.037153\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.065788\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.017372\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.036683\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.008751\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.048356\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.083345\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.138003\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.127195\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.016302\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.065331\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.033218\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.037449\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.068976\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.140503\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.018459\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.039934\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.041177\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.068007\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.106497\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.156638\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.058302\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.076541\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.009349\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.051930\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.070959\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.121129\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.021178\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.037392\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.072475\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.072550\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.040984\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.006470\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.008790\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.006779\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.059235\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.005999\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.050632\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.029035\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.081762\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.013425\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.171009\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.014047\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.081378\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.077472\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.048042\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.030221\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.050488\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.059097\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.111273\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.112535\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.026033\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.023518\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.005986\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.006051\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.024997\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.017219\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.059605\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.004924\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.071566\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.004160\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.025700\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.074390\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.043603\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.003918\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.037602\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.033831\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.107070\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.284425\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.060968\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.092520\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.037789\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.019783\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.040096\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.022813\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.003961\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.021271\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.044754\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.104862\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.148382\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.035891\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.022071\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.066667\n",
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "finish epoch 3\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.034554\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.006751\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.021151\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.016671\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.020248\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.038504\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.004122\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.073151\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.002685\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.016228\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.008110\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.035888\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.002537\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.076935\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.002024\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.006642\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.052453\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.005911\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.019399\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.017193\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.097946\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.020926\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.003524\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.029251\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.017371\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.014135\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.039980\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.041645\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.114712\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.011207\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005332\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.046964\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.063296\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.002261\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.082830\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.001599\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.002070\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.009362\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.014515\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.011287\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.006719\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.054047\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.112960\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.127012\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.018866\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.002776\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.002815\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.043174\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.006272\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.028951\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.122959\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.019340\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.101226\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.049465\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.009325\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.005422\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.005443\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.106540\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.004754\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.064522\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.009571\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.001485\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.005266\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.022977\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.011971\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.002199\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.017207\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.062161\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.011694\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.014529\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.047582\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.023344\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.099054\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.063608\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.140571\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.076742\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.015809\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.004274\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.012823\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.004936\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.009045\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.056971\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.004180\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.007255\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.074776\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.094553\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.032018\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.011277\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.020070\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.034691\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.061788\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.106470\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.106049\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.007359\n",
      "\n",
      "Test set: Average loss: 0.0504, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "finish epoch 4\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.052428\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.096964\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.048841\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.000958\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.004849\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.075122\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.004245\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.017034\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.027346\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.033436\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.047720\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.011133\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.036869\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.020730\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001266\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.010246\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.052076\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.017009\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.034135\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.034599\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.006678\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.009870\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.027617\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.011700\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.010837\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.003032\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.060659\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.016141\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.059222\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.001563\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009210\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.006681\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.146632\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.033866\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.002117\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.078362\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.117939\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.012372\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.019239\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.010980\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024675\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.001115\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.138756\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.002053\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.057374\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.014516\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.086200\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.001573\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.003780\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.005334\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.002106\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.034109\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.041616\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.001965\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.010228\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.038894\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.014679\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.029399\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.025376\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.021017\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.013738\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.108258\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.003096\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.044710\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.009269\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.030580\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.057421\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.021080\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.004595\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.022002\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.002839\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.104457\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.010441\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.203742\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.001451\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.007622\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.004581\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.011015\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.082271\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.021422\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.053849\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.001814\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.049021\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.026556\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.013394\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.000888\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.021254\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.002708\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.003171\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.066518\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.034433\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.017071\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.065286\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.018795\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "finish epoch 5\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.097048\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.006060\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.001238\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.002911\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.021184\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.095183\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.004592\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.014851\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.001501\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.001219\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.006580\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.059443\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.010071\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.007856\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.006776\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.037586\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.045108\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.202471\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.004485\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.004962\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.001862\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.038666\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.048081\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.028521\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.090429\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.003790\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.022765\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.002475\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.045467\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.044485\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.006021\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.002059\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.037762\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.009245\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.003812\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.000655\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.001507\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.041443\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.013802\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.033759\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.004254\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.030837\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.096922\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.067030\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.002444\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.029299\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.038979\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.003881\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.046128\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.030721\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012221\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.002027\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.015995\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.231083\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.076536\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.019868\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.007739\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.057189\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.003274\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.045080\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.090477\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.004290\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.016564\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.005208\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.032736\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.006705\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.109150\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.012336\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.017579\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.011470\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000523\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.003150\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.067046\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.017243\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.091401\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.022781\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.027138\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.042252\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.010933\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.002295\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001866\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.004159\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.030624\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.002297\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.016093\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.047730\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.002314\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.003426\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.110493\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.099825\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.181120\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.009813\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.127603\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.003599\n",
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "finish epoch 6\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.009241\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.011675\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.003610\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.010781\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.090265\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.029013\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.000528\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.004027\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.015873\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.004595\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.012711\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.004452\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.005388\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.003262\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.066657\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.000973\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.002269\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.015698\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.002162\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.132345\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004119\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.015039\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.011224\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.002099\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.002393\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.004058\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.007667\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.048048\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.008084\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.005080\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.003984\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.066188\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.016202\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.017693\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.033090\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.010086\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.002990\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.016797\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.102917\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.023366\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.020022\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.014201\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.005565\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.001243\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.006769\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.003651\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.002521\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.001426\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.025407\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.005700\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.005833\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.025133\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.011445\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.011129\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.005058\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.065597\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.018013\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.008783\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.006070\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.002564\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.010144\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.008529\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.017595\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.004090\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.002812\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.003669\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.011366\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.104409\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.003839\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.017454\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004435\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.001472\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.002881\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.011810\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.000824\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.003172\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.002521\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.018577\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.011687\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.000693\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.005523\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.029469\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.002033\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.003603\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.007108\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.000424\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.006616\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.002530\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.070256\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.012070\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.005148\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.063507\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.063089\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.002606\n",
      "\n",
      "Test set: Average loss: 0.0361, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "finish epoch 7\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.087165\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.010749\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.011368\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.009193\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001026\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.000447\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.021160\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.001754\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.000582\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.071835\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002140\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.055752\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.003013\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.012177\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.001609\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.004709\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.001314\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.002249\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.000629\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.004191\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.023997\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.002934\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.026648\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.009680\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.026917\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.003593\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.006543\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.028429\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.001786\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.001482\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001009\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.090976\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.015536\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.001281\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001247\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.004081\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.011762\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.001710\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.067953\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.001008\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001294\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.056169\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.005212\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.043625\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.025694\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.054151\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.025978\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.001101\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.037283\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.013356\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006481\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.010023\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.024225\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.008015\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.002917\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.012180\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.003390\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.016003\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.066182\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.001390\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000506\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.009317\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.001522\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.006242\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.023773\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.001102\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.004695\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.000441\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.003819\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.009860\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.051955\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.090738\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.009161\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.001615\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.002059\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.022743\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.004327\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.007715\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.020558\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.003113\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.018987\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.022422\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.005257\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.021929\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.012625\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.015104\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.030107\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.082146\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.010051\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.004396\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005349\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.028982\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.009785\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.002708\n",
      "\n",
      "Test set: Average loss: 0.0339, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "finish epoch 8\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.006677\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.001278\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.090320\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.008901\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.007601\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.001259\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.016112\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.012906\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.001493\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.024974\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000504\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.030056\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.001617\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.001797\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.005647\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.000571\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.003619\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.012903\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.003173\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.000697\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.040653\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.008743\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.002836\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.001145\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001119\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.008554\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.000430\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.001009\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.012862\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.005778\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.025421\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.013521\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.003873\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.014436\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.025260\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.001989\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.004510\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.005248\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.094845\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.001052\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001372\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.046094\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.000766\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.002545\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.000927\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.000897\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.007966\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.008906\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.008102\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.001543\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000477\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.002835\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.001238\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.040210\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.001488\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.003602\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.000700\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.019418\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.001042\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.019136\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.040077\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.000524\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.014267\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.009079\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.007785\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.019230\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.001869\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.005714\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.011991\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.002425\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002195\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.002164\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.001093\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.008073\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.001427\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.021018\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.001517\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.056155\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.001169\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.007574\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.023887\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.030534\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.029874\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.062448\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.003681\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.018862\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.010434\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.000492\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.002240\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.032134\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.002226\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.036835\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.012689\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.027160\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "finish epoch 9\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001361\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.001430\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.006025\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.001143\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.004743\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.016528\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001322\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.011484\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.010936\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.000606\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.044890\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.010067\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.028690\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.014689\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.003607\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.001522\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.006708\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.005929\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.008761\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.021912\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.082904\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.036453\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.002011\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.001534\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.002172\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.006518\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.002603\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.031910\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.004960\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.003852\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002164\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.001598\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.005336\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.000923\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.022974\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.003333\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.032110\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.003163\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.017808\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.037163\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001922\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.003076\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.000793\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.044614\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.002320\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.002136\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.005271\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.002068\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.008350\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.078920\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001313\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.004390\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.014932\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.000738\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.032543\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.000721\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.021296\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.025779\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.010057\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.006336\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.073178\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.016781\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.012035\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.001816\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.025495\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.001381\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.000738\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.013966\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.000894\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.001909\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.071911\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.003131\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.053645\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.088383\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.066177\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.000480\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.145585\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.007594\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.004402\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.050292\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001134\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.048894\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.025351\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.053805\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.037208\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.003846\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.010452\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.006447\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.007012\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.000242\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001638\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.004033\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.001377\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.003678\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "finish epoch 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Assignment_2_Part_2_RNN_MNIST_vp1.ipynb\n",
    "Overall structure:\n",
    "\n",
    "1) Set Pytorch metada\n",
    "- seed\n",
    "- tensorflow output\n",
    "- whether to transfer to gpu (cuda)\n",
    "\n",
    "2) Import data\n",
    "- download data\n",
    "- create data loaders with batchsie, transforms, scaling\n",
    "\n",
    "3) Define Model architecture, loss and optimizer\n",
    "\n",
    "4) Define Test and Training loop\n",
    "    - Train:\n",
    "        a. get next batch\n",
    "        b. forward pass through model\n",
    "        c. calculate loss\n",
    "        d. backward pass from loss (calculates the gradient for each parameter)\n",
    "        e. optimizer: performs weight updates\n",
    "\n",
    "5) Perform Training over multiple epochs:\n",
    "    Each epoch:\n",
    "    - call train loop\n",
    "    - call test loop\n",
    "\n",
    "# Step 1: Pytorch and Training Metadata\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "# 1) setting up the logging\n",
    "\n",
    "datetime_str = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "if logging_dir is None:\n",
    "    runs_dir = Path(\"./\") / Path(f\"runs/\")\n",
    "    runs_dir.mkdir(exist_ok = True)\n",
    "\n",
    "    logging_dir = runs_dir / Path(f\"{datetime_str}\")\n",
    "\n",
    "    logging_dir.mkdir(exist_ok = True)\n",
    "    logging_dir = str(logging_dir.absolute())\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)\n",
    "\n",
    "#deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "\"\"\"# Step 2: Data Setup\"\"\"\n",
    "\n",
    "# Setting up data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# plot one example\n",
    "print(train_dataset.data.size())     # (60000, 28, 28)\n",
    "print(train_dataset.targets.size())   # (60000)\n",
    "plt.imshow(train_dataset.data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_dataset.targets[0])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"# Step 3: Creating the Model\"\"\"\n",
    "\n",
    "\"\"\"# Step 3: Creating the Model\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Using GRU instead of RNN\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=128,     # number of hidden units\n",
    "            num_layers=2,       # number of layers\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(128, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GRU returns output, h_n\n",
    "        r_out, h_n = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ... [rest of the code remains unchanged]\n",
    "\n",
    "\n",
    "\"\"\"# Step 4: Train/Test\"\"\"\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data = data.view(-1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            writer.add_scalar('training_loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "    writer.add_scalar('test_accuracy', 100. * correct / len(test_loader.dataset), epoch)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    print('finish epoch',epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501e9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c2b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 78749), started 0:22:02 ago. (Use '!kill 78749' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c1c57761f1c78e4d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c1c57761f1c78e4d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d18c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
